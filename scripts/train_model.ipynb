{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare environment and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set current working directory to the src folder\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            K           P        pH   Moisture     Crop Phenological_Stage  \\\n",
      "0  144.405994  109.567066  6.639521  55.157729    Wheat          Flowering   \n",
      "1  170.956093   84.244906  6.214310  26.138757    Wheat           Maturity   \n",
      "2  113.754108   83.131374  5.993584  44.713710  Soybean           Maturity   \n",
      "3  207.123230   98.960434  7.067170  38.428817    Wheat                 R6   \n",
      "4  185.330043   99.104843  6.547563  49.969815    Wheat           Maturity   \n",
      "\n",
      "   Irrigate  \n",
      "0     False  \n",
      "1      True  \n",
      "2      True  \n",
      "3     False  \n",
      "4     False  \n",
      "                K           P          pH    Moisture\n",
      "count  500.000000  500.000000  500.000000  500.000000\n",
      "mean   201.497700  102.214548    6.495519   40.783996\n",
      "std     50.712149   28.974022    0.454040   15.108114\n",
      "min    100.000000   50.000000    5.500000   10.000000\n",
      "25%    162.205225   81.309135    6.199047   30.002131\n",
      "50%    204.954945  101.401257    6.475943   41.714489\n",
      "75%    235.988889  122.906502    6.824187   50.845115\n",
      "max    350.000000  180.000000    7.500000   80.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_fake_irrigation_data(num_samples=100, noise_probability=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Generates a realistic dataset for irrigation recommendations using Gaussian distributions.\n",
    "\n",
    "    Args:\n",
    "        num_samples: The number of samples to generate.\n",
    "        noise_probability: Probability of adding noise to irrigation decisions.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    crops = ['Corn', 'Soybean', 'Wheat']\n",
    "    phenological_stages = ['V6', 'R1', 'R6', 'Flowering', 'Maturity']\n",
    "\n",
    "    data = {\n",
    "        'K': [],\n",
    "        'P': [],\n",
    "        'pH': [],\n",
    "        'Moisture': [],\n",
    "        'Crop': [],\n",
    "        'Phenological_Stage': [],\n",
    "        'Irrigate': []\n",
    "    }\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        crop = np.random.choice(crops)\n",
    "        stage = np.random.choice(phenological_stages)\n",
    "\n",
    "        # Gaussian distributions for sensor readings (adjust means and std devs as needed)\n",
    "        k = np.random.normal(loc=200, scale=50)  # Potassium (mg/kg) - Mean 200, Std Dev 50\n",
    "        p = np.random.normal(loc=100, scale=30)  # Phosphorus (mg/kg) - Mean 100, Std Dev 30\n",
    "        ph = np.random.normal(loc=6.5, scale=0.5) # Soil pH - Mean 6.5, Std Dev 0.5\n",
    "        moisture = np.random.normal(loc=40, scale=15)  # Soil moisture (%) - Mean 40, Std Dev 15\n",
    "\n",
    "\n",
    "        # Clip values to realistic ranges (important for Gaussian distributions)\n",
    "        k = np.clip(k, 100, 350)  \n",
    "        p = np.clip(p, 50, 180)\n",
    "        ph = np.clip(ph, 5.5, 7.5)\n",
    "        moisture = np.clip(moisture, 10, 80)\n",
    "\n",
    "\n",
    "        irrigate = False\n",
    "\n",
    "        # Crop-specific irrigation logic (including K, P, and pH)\n",
    "        if crop == 'Corn':\n",
    "            if stage in ['R1', 'R6']:\n",
    "                if moisture < 30 or k < 150 or p < 75 or ph < 6.0 or ph > 7.0:\n",
    "                    irrigate = True\n",
    "            elif moisture < 20 or k < 100 or p < 50:\n",
    "                irrigate = True\n",
    "\n",
    "        elif crop == 'Soybean':\n",
    "            if stage == 'Flowering':\n",
    "                if moisture < 40 or k < 180 or p < 80 or ph < 6.2 or ph > 7.2:\n",
    "                    irrigate = True\n",
    "            elif moisture < 25 or k < 120 or p < 60:\n",
    "                irrigate = True\n",
    "\n",
    "        elif crop == 'Wheat':\n",
    "            if stage == 'Maturity':\n",
    "                if moisture < 35 or k < 120 or p < 70 or ph < 5.8 or ph > 6.8:\n",
    "                    irrigate = True\n",
    "            elif moisture < 20 or k < 100 or p < 55:\n",
    "                irrigate = True\n",
    "\n",
    "        # Add noise (near boundaries)\n",
    "        near_boundary = False\n",
    "        if (moisture < 25 or k < 100 or p < 50 or ph < 5.8 or ph > 7.2): near_boundary = True\n",
    "\n",
    "        if near_boundary and np.random.rand() < noise_probability:\n",
    "            irrigate = not irrigate\n",
    "\n",
    "        data['K'].append(k)\n",
    "        data['P'].append(p)\n",
    "        data['pH'].append(ph)\n",
    "        data['Moisture'].append(moisture)\n",
    "        data['Crop'].append(crop)\n",
    "        data['Phenological_Stage'].append(stage)\n",
    "        data['Irrigate'].append(irrigate)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "df = generate_fake_irrigation_data(num_samples=500, noise_probability=0.10)\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "#df.to_csv('irrigation_data_gaussian.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preproccess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data normalization (for neural features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          K         P        pH  Moisture     Crop Phenological_Stage  \\\n",
      "0 -1.126927  0.254017  0.317475  0.952344    Wheat          Flowering   \n",
      "1 -0.602857 -0.620820 -0.619969 -0.970333    Wheat           Maturity   \n",
      "2 -1.731961 -0.659290 -1.106593  0.260367  Soybean           Maturity   \n",
      "3  0.111042 -0.112424  1.260294 -0.156044    Wheat                 R6   \n",
      "4 -0.319132 -0.107435  0.114740  0.608615    Wheat           Maturity   \n",
      "\n",
      "   Irrigate  \n",
      "0     False  \n",
      "1      True  \n",
      "2      True  \n",
      "3     False  \n",
      "4     False  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_features(data, columns):\n",
    "    \"\"\"\n",
    "    Normalizes the specified numerical features in the given data.\n",
    "\n",
    "    Args:\n",
    "        data: The input data as a pandas DataFrame.\n",
    "        columns: The list of columns to normalize.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with normalized features.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    data[columns] = scaler.fit_transform(data[columns])\n",
    "    return data\n",
    "\n",
    "\n",
    "# Normalize the numerical features\n",
    "df_normalized = normalize_features(df, ['K', 'P', 'pH', 'Moisture'])\n",
    "print(df_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. One-hot encoding (for categorical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          K         P        pH  Moisture  Irrigate  Crop_Corn  Crop_Soybean  \\\n",
      "0 -1.126927  0.254017  0.317475  0.952344     False      False         False   \n",
      "1 -0.602857 -0.620820 -0.619969 -0.970333      True      False         False   \n",
      "2 -1.731961 -0.659290 -1.106593  0.260367      True      False          True   \n",
      "3  0.111042 -0.112424  1.260294 -0.156044     False      False         False   \n",
      "4 -0.319132 -0.107435  0.114740  0.608615     False      False         False   \n",
      "\n",
      "   Crop_Wheat  Phenological_Stage_Flowering  Phenological_Stage_Maturity  \\\n",
      "0        True                          True                        False   \n",
      "1        True                         False                         True   \n",
      "2       False                         False                         True   \n",
      "3        True                         False                        False   \n",
      "4        True                         False                         True   \n",
      "\n",
      "   Phenological_Stage_R1  Phenological_Stage_R6  Phenological_Stage_V6  \n",
      "0                  False                  False                  False  \n",
      "1                  False                  False                  False  \n",
      "2                  False                  False                  False  \n",
      "3                  False                   True                  False  \n",
      "4                  False                  False                  False  \n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_features(data, columns):\n",
    "    \"\"\"\n",
    "    One-hot encodes the specified categorical features in the given data.\n",
    "\n",
    "    Args:\n",
    "        data: The input data as a pandas DataFrame.\n",
    "        columns: The list of columns to one-hot encode.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with one-hot encoded features.\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(data, columns=columns)\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "categorical_columns = ['Crop', 'Phenological_Stage']\n",
    "df_normalized = one_hot_encode_features(df_normalized, categorical_columns)\n",
    "print(df_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "            K         P        pH  Moisture  Irrigate  Crop_Corn  \\\n",
      "249 -0.956635 -0.225848 -1.323601  0.544288     False      False   \n",
      "433  0.422233  1.659014  0.003711  0.612677     False       True   \n",
      "19  -0.924709 -0.923653 -0.522680 -0.618262     False       True   \n",
      "322  0.818476  0.282286 -1.710734 -1.471176      True       True   \n",
      "332  1.176286  1.736840 -1.108343  0.726102     False       True   \n",
      "\n",
      "     Crop_Soybean  Crop_Wheat  Phenological_Stage_Flowering  \\\n",
      "249         False        True                          True   \n",
      "433         False       False                         False   \n",
      "19          False       False                         False   \n",
      "322         False       False                         False   \n",
      "332         False       False                         False   \n",
      "\n",
      "     Phenological_Stage_Maturity  Phenological_Stage_R1  \\\n",
      "249                        False                  False   \n",
      "433                        False                  False   \n",
      "19                         False                  False   \n",
      "322                        False                   True   \n",
      "332                         True                  False   \n",
      "\n",
      "     Phenological_Stage_R6  Phenological_Stage_V6  \n",
      "249                  False                  False  \n",
      "433                  False                   True  \n",
      "19                    True                  False  \n",
      "322                  False                  False  \n",
      "332                  False                  False  \n",
      "Testing Data:\n",
      "            K         P        pH  Moisture  Irrigate  Crop_Corn  \\\n",
      "361  1.057023  1.953215 -1.345487  0.441694     False      False   \n",
      "73   0.702713  1.453970  0.781777 -0.183429     False       True   \n",
      "374  0.411995 -0.859548 -1.204513  0.580428      True      False   \n",
      "155  1.986169  0.426350  0.170044  1.651071     False      False   \n",
      "104  0.452109  1.561217 -0.122235  0.061084     False       True   \n",
      "\n",
      "     Crop_Soybean  Crop_Wheat  Phenological_Stage_Flowering  \\\n",
      "361         False        True                         False   \n",
      "73          False       False                         False   \n",
      "374          True       False                          True   \n",
      "155         False        True                         False   \n",
      "104         False       False                         False   \n",
      "\n",
      "     Phenological_Stage_Maturity  Phenological_Stage_R1  \\\n",
      "361                         True                  False   \n",
      "73                         False                  False   \n",
      "374                        False                  False   \n",
      "155                        False                  False   \n",
      "104                        False                   True   \n",
      "\n",
      "     Phenological_Stage_R6  Phenological_Stage_V6  \n",
      "361                  False                  False  \n",
      "73                    True                  False  \n",
      "374                  False                  False  \n",
      "155                  False                   True  \n",
      "104                  False                  False  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(df_normalized, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(train_data.head())\n",
    "print(\"Testing Data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float types\n",
    "train_data = train_data.astype(float)\n",
    "test_data = test_data.astype(float)\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_data.drop(columns=['Irrigate'])\n",
    "y_train = train_data['Irrigate']\n",
    "X_test = test_data.drop(columns=['Irrigate'])\n",
    "y_test = test_data['Irrigate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Auxiliary functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes various classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true: The ground truth target labels.\n",
    "        y_pred: The predicted target labels.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of metric names and values.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "        'precision': float(precision_score(y_true, y_pred)),\n",
    "        'recall': float(recall_score(y_true, y_pred)),\n",
    "        'f1': float(f1_score(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.88, 'precision': 0.84, 'recall': 0.7241379310344828, 'f1': 0.7777777777777778}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the neural network model\n",
    "model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = get_metrics(y_test, y_pred)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.86, 'precision': 0.7777777777777778, 'recall': 0.7241379310344828, 'f1': 0.75}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_metrics = get_metrics(y_test, y_rf_pred)\n",
    "print(rf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.78, 'precision': 0.6842105263157895, 'recall': 0.4482758620689655, 'f1': 0.5416666666666666}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "log_reg_model = LogisticRegression(random_state=42, max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_log_reg_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "log_reg_metrics = get_metrics(y_test, y_log_reg_pred)\n",
    "print(log_reg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.86, 'precision': 0.8571428571428571, 'recall': 0.6206896551724138, 'f1': 0.72}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the KNN modelp\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "knn_metrics = get_metrics(y_test, y_knn_pred)\n",
    "print(knn_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.77, 'precision': 0.65, 'recall': 0.4482758620689655, 'f1': 0.5306122448979592}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "svm_metrics = get_metrics(y_test, y_svm_pred)\n",
    "print(svm_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.78, 'precision': 0.7058823529411765, 'recall': 0.41379310344827586, 'f1': 0.5217391304347826}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define the Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_nb_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "nb_metrics = get_metrics(y_test, y_nb_pred)\n",
    "print(nb_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Neural Network' saved to database with ID: 1\n",
      "Model 'Random Forest' saved to database with ID: 2\n",
      "Model 'Logistic Regression' saved to database with ID: 3\n",
      "Model 'K-Nearest Neighbors' saved to database with ID: 4\n",
      "Model 'Support Vector Machine' saved to database with ID: 5\n",
      "Model 'Naive Bayes' saved to database with ID: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from db.ml_model_crud import create_ml_model # Assuming your CRUD functions are in db.crud\n",
    "\n",
    "def save_trained_model(model, model_name, model_type, accuracy, precision, recall, f1_score, ml_library=\"scikit-learn\"):\n",
    "    \"\"\"\n",
    "    Saves a trained model and its metrics to the database.\n",
    "\n",
    "    Args:\n",
    "        model: The trained scikit-learn model object.  (Not used for saving, only for potential later retrieval)\n",
    "        model_name: Name of the model (string).\n",
    "        model_type: Type of model (e.g., \"Classification\", \"Regression\").\n",
    "        accuracy: Accuracy score.\n",
    "        precision: Precision score.\n",
    "        recall: Recall score.\n",
    "        f1_score: F1-score.\n",
    "        ml_library: The machine learning library used (default: \"scikit-learn\").\n",
    "\n",
    "    Returns:\n",
    "        The database ID of the newly created MLModel entry, or None if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    model_dir = os.path.join(os.getcwd(), '../src/models')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    try:\n",
    "        #We are NOT pickling the model here. Only saving meta-information.\n",
    "        # Save the model to a file\n",
    "        model_path = os.path.join(model_dir, f\"{model_name.replace(' ', '_').lower()}_model.pkl\")\n",
    "        with open(model_path, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "        # Save model parameters (this would need to be adjusted based on how your models store parameters)\n",
    "        try:\n",
    "            model_parameters = str(model.get_params())  # Convert to string for database storage\n",
    "        except AttributeError:\n",
    "            model_parameters = \"Parameters not available\" #Handle models without get_params\n",
    "\n",
    "        # Save to database\n",
    "        new_model = create_ml_model(\n",
    "            model_name=model_name,\n",
    "            model_type=model_type,\n",
    "            model_parameters=model_parameters,  # Store model parameters as a string\n",
    "            ml_library=ml_library,\n",
    "            accuracy=accuracy,\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            f1_score=f1_score,\n",
    "        )\n",
    "\n",
    "        print(f\"Model '{model_name}' saved to database with ID: {new_model.id_model}\")\n",
    "        return new_model.id_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Save the trained models to the database\n",
    "save_trained_model(model, \"Neural Network\", \"Classification\", metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1'])\n",
    "save_trained_model(rf_model, \"Random Forest\", \"Classification\", rf_metrics['accuracy'], rf_metrics['precision'], rf_metrics['recall'], rf_metrics['f1'])\n",
    "save_trained_model(log_reg_model, \"Logistic Regression\", \"Classification\", log_reg_metrics['accuracy'], log_reg_metrics['precision'], log_reg_metrics['recall'], log_reg_metrics['f1'])\n",
    "save_trained_model(knn_model, \"K-Nearest Neighbors\", \"Classification\", knn_metrics['accuracy'], knn_metrics['precision'], knn_metrics['recall'], knn_metrics['f1'])\n",
    "save_trained_model(svm_model, \"Support Vector Machine\", \"Classification\", svm_metrics['accuracy'], svm_metrics['precision'], svm_metrics['recall'], svm_metrics['f1'])\n",
    "save_trained_model(nb_model, \"Naive Bayes\", \"Classification\", nb_metrics['accuracy'], nb_metrics['precision'], nb_metrics['recall'], nb_metrics['f1'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
